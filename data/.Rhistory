scale_size_area(max_size = 10, guide = FALSE) +
scale_color_manual(values = party_pal, name = "", breaks = c("Democratic","Republican","Whig","Democratic-Republican","Federalist","None")) +
scale_y_continuous(limits = c(20,90)) +
theme_minimal(base_size = 24, base_family = "ProximaNova-Semibold") +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=author, size = words)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_color_manual(values = col_pal, name = "", breaks = c("Democratic","Republican","Whig","Democratic-Republican","Federalist","None")) +
scale_y_continuous(limits = c(20,90)) +
theme_minimal(base_size = 24, base_family = "ProximaNova-Semibold") +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=author, size = words)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_color_manual(values = col_pal, name = "", breaks = c("Democratic","Republican","Whig","Democratic-Republican","Federalist","None")) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=author, size = words)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
View(sentiments_counts)
View(sentiments)
View(sentiments_counts)
sentiments_counts <- sentiments %>%
group_by(headline, date) %>%
count(sentiment) %>%
arrange(-n)
View(sentiments_counts)
positive_freqs <- sentiments_counts %>%
left_join(sentiments_counts %>%
group_by(headline,date) %>%
summarise(total = sum(n))) %>%
mutate(percent = round(n/total*100,2)) %>%
filter(sentiment == "positive")
View(sentiments_counts)
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=author, size = words)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=heaadline, size = words)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=headline, size = words)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=headline)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=headline, size=5)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
library(ggthemes)
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=headline, size=5)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme_wsj() +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=headline, size=5)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
scale_fill_discrete(guide=FALSE) +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme_wsj() +
theme(legend.position=c(0.7,0.2),
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=headline, size=5)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme_wsj() +
theme(legend.position="none",
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
sentiments_counts <- sentiments %>%
group_by(headline, date, author) %>%
count(sentiment) %>%
arrange(-n)
positive_freqs <- sentiments_counts %>%
left_join(sentiments_counts %>%
group_by(headline,date,author) %>%
summarise(total = sum(n))) %>%
mutate(percent = round(n/total*100,2)) %>%
filter(sentiment == "positive")
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=author, size=5)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme_wsj() +
theme(legend.position="none",
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=author, size=5)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme_wsj() +
theme(
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
#ingest data
data <- read_csv("feature_winners_2007_2017.csv")
#Necessary to avoid encoding errors when converting to lowercase
data$graf <- sapply(data$graf,function(row) iconv(row, "latin1", "ASCII", sub=""))
#lowercase is needed for nsyllable, which bugs out when encountering ALL CAPS
data$lower_graf <- tolower(data$graf)
#adds fields for number of syllables, number of words, number of sentences,
#Flesch-Kincaid Reading Ease, and Flesch-Kincaid Reading Level.
#This is per graf data, so later I'll calculate it for the whole articles.
data <- data %>%
mutate(year = year(date),
syllables = nsyllable(lower_graf),
sentences = nsentence(graf),
words = ntoken(graf, remove_punct = TRUE),
fk_ease = 206.835 - 1.105*(words/sentences) - 84.6*(syllables/words),
fk_grade = 0.39*(words/sentences) + 11.8*(syllables/words) - 15.59) %>%
arrange(date)
#create new data frame with summed values of syllable count, word count, and sentence count
summary_data <- data.frame(data$headline, data$syllables, data$sentences, data$words, data$year_won)
summary_data <- aggregate(summary_data[2:4], list(summary_data$data.headline), FUN=sum)
summary_data <- summary_data %>%
mutate(fk_ease = 206.835 - 1.105*(data.words/data.sentences) - 84.6*(data.syllables/data.words),
fk_grade = 0.39*(data.words/data.sentences) + 11.8*(data.syllables/data.words) - 15.59)
names(summary_data)[names(summary_data) == 'Group.1'] <- 'headline'
summary_data <- merge(x=summary_data, data[ ,c('headline','date')], by='headline', all.x=TRUE)
summary_data <- unique(summary_data)
#Makes a cute box plot
ggplot(summary_data, aes(x=reorder(headline, date), y=fk_grade), color=NA) +
geom_bar(stat='identity',color="#909090", fill="blue") +
ylab("Reading Level") +
xlab("") +
ggtitle("Pulitzer Winning Feature Reading Grades", subtitle="Winners from 2007-2017") +
coord_flip() +
scale_y_continuous(limits=c(0,15), breaks = c(0,5,10,15,20)) +
theme_minimal(base_size=25, base_family="Garamond") +
theme(axis.title.x=element_blank(),
legend.position="none",
axis.ticks.x=element_blank(),
legend.text = element_text(color="#909090", size = 10))
ggsave("box_plot.png",width=18, height=7, units = "in", dpi = 300)
ggplot(data, aes(x=graf_id, y=fk_grade, color=headline)) +
geom_point(aes(color=headline, fill=headline), alpha=.8) +
ylab("Reading Level") +
xlab("") +
theme_minimal() +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
# load lexicon from https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html
bing <- get_sentiments("bing")
# sentiment by paragraph
sentiments <- data %>%
unnest_tokens(word, graf) %>%
filter(str_detect(word, "[a-z]")) %>%
# match to lexicon
inner_join(bing, by = "word")
sentiments_counts <- sentiments %>%
group_by(headline, date, author) %>%
count(sentiment) %>%
arrange(-n)
positive_freqs <- sentiments_counts %>%
left_join(sentiments_counts %>%
group_by(headline,date,author) %>%
summarise(total = sum(n))) %>%
mutate(percent = round(n/total*100,2)) %>%
filter(sentiment == "positive")
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=author, size=5)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme_wsj() +
theme(
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
View(data)
data <- read_csv("feature_winners_2007_2017.csv")
#Necessary to avoid encoding errors when converting to lowercase
data$graf <- sapply(data$graf,function(row) iconv(row, "latin1", "ASCII", sub=""))
#lowercase is needed for nsyllable, which bugs out when encountering ALL CAPS
data$lower_graf <- tolower(data$graf)
#adds fields for number of syllables, number of words, number of sentences,
#Flesch-Kincaid Reading Ease, and Flesch-Kincaid Reading Level.
#This is per graf data, so later I'll calculate it for the whole articles.
data <- data %>%
mutate(year = year(date),
syllables = nsyllable(lower_graf),
sentences = nsentence(graf),
words = ntoken(graf, remove_punct = TRUE),
fk_ease = 206.835 - 1.105*(words/sentences) - 84.6*(syllables/words),
fk_grade = 0.39*(words/sentences) + 11.8*(syllables/words) - 15.59) %>%
arrange(date)
#create new data frame with summed values of syllable count, word count, and sentence count
summary_data <- data.frame(data$headline, data$syllables, data$sentences, data$words, data$year_won)
summary_data <- aggregate(summary_data[2:4], list(summary_data$data.headline), FUN=sum)
summary_data <- summary_data %>%
mutate(fk_ease = 206.835 - 1.105*(data.words/data.sentences) - 84.6*(data.syllables/data.words),
fk_grade = 0.39*(data.words/data.sentences) + 11.8*(data.syllables/data.words) - 15.59)
names(summary_data)[names(summary_data) == 'Group.1'] <- 'headline'
summary_data <- merge(x=summary_data, data[ ,c('headline','date')], by='headline', all.x=TRUE)
summary_data <- unique(summary_data)
#Makes a cute box plot
ggplot(summary_data, aes(x=reorder(headline, date), y=fk_grade), color=NA) +
geom_bar(stat='identity',color="#909090", fill="blue") +
ylab("Reading Level") +
xlab("") +
ggtitle("Pulitzer Winning Feature Reading Grades", subtitle="Winners from 2007-2017") +
coord_flip() +
scale_y_continuous(limits=c(0,15), breaks = c(0,5,10,15,20)) +
theme_minimal(base_size=25, base_family="Garamond") +
theme(axis.title.x=element_blank(),
legend.position="none",
axis.ticks.x=element_blank(),
legend.text = element_text(color="#909090", size = 10))
ggsave("box_plot.png",width=18, height=7, units = "in", dpi = 300)
ggplot(data, aes(x=graf_id, y=fk_grade, color=headline)) +
geom_point(aes(color=headline, fill=headline), alpha=.8) +
ylab("Reading Level") +
xlab("") +
theme_minimal() +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
# load lexicon from https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html
bing <- get_sentiments("bing")
# sentiment by paragraph
sentiments <- data %>%
unnest_tokens(word, graf) %>%
filter(str_detect(word, "[a-z]")) %>%
# match to lexicon
inner_join(bing, by = "word")
sentiments_counts <- sentiments %>%
group_by(headline, date, author) %>%
count(sentiment) %>%
arrange(-n)
positive_freqs <- sentiments_counts %>%
left_join(sentiments_counts %>%
group_by(headline,date,author) %>%
summarise(total = sum(n))) %>%
mutate(percent = round(n/total*100,2)) %>%
filter(sentiment == "positive")
# sentiment chart
ggplot(positive_freqs, aes(x=date, y=percent, color=author, size=5)) +
geom_point(alpha=0.5) +
geom_smooth(se=F, color="black", method="lm", size=0.5, linetype = "dotted") +
scale_size_area(max_size = 10, guide = FALSE) +
scale_y_continuous(limits = c(20,90)) +
xlab("") +
ylab("% positive words") +
guides(col = guide_legend(ncol = 2, override.aes = list(size = 4))) +
theme_wsj() +
theme(
legend.text = element_text(color="#909090", size = 18),
panel.grid.minor = element_blank())
# break text into bigrams
bigrams <- data %>%
unnest_tokens(bigram, graf, token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(president,party,date,year,words) %>%
count(bigram) %>%
arrange(-n)
# break text into bigrams
bigrams <- data %>%
unnest_tokens(bigram, graf, token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(headline,author,date,year,words) %>%
count(bigram) %>%
arrange(-n)
View(bigrams)
bigram_freqs <- bigrams %>%
left_join(bigrams %>%
group_by(headline,author,date,year,words) %>%
summarise(total = sum(n))) %>%
mutate(percent = n/total*100) %>%
group_by(president,party,date)
bigram_freqs <- bigrams %>%
left_join(bigrams %>%
group_by(headline,author,date,year,words) %>%
summarise(total = sum(n))) %>%
mutate(percent = n/total*100) %>%
group_by(headline,author,date)
View(bigram_freqs)
full_articles <- aggregate(graf ~ headline, data=data, paste, sep=",")
View(full_articles)
write.csv(full_articles, "full_articles.csv")
write.table(full_articles, "full_articles.txt")
full_articles <- aggregate(graf ~ headline + date, data=data, paste, sep=",")
View(full_articles)
# break text into bigrams
bigrams <- full_articles %>%
unnest_tokens(bigram, graf, token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(headline,author,date) %>%
count(bigram) %>%
arrange(-n)
full_articles$graf <- unlist(full_articles$graf, use.names=FALSE)
article_text<- ad.data.frame(full_articles$graf)
article_text<- as.data.frame(full_articles$graf)
article_text<- data.frame(graf = unlist(full_articles$graf))
View(article_text)
full_articles <- aggregate(graf ~ headline + date, data=data, sep=",")
full_articles <- aggregate(graf ~ headline + date, data=data, c)
View(article_text)
View(full_articles)
full_articles <- aggregate(graf ~ headline + date, data=data, paste, collapse = ",")
View(full_articles)
bigr.getMaxStringLength()
full_articles <- aggregate(graf ~ headline + date, data=data, paste, sep=",")
# break text into bigrams
bigrams <- full_articles %>%
unnest_tokens(bigram, graf, token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(headline,author,date) %>%
count(bigram) %>%
arrange(-n)
# break text into bigrams
bigrams <- full_articles %>%
unnest_tokens(bigram, collapse = FALSE, graf, token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(headline,author,date) %>%
count(bigram) %>%
arrange(-n)
# break text into bigrams
bigrams <- full_articles %>%
unnest_tokens_(bigram, graf, token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(headline,author,date) %>%
count(bigram) %>%
arrange(-n)
# break text into bigrams
bigrams <- full_articles %>%
unnest_tokens_(bigram, graf, token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(headline,author,date) %>%
count(bigram) %>%
arrange(-n)
# break text into bigrams
bigrams <- full_articles %>%
unnest_tokens(bigram, graf, token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(headline,author,date) %>%
count(bigram) %>%
arrange(-n)
# break text into bigrams
bigrams <- data %>%
unnest_tokens(bigram, graf, token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(headline,author,date) %>%
count(bigram) %>%
arrange(-n)
# break text into bigrams
bigrams <- data %>%
unnest_tokens(bigram, aggregate(graf~headline), token = "ngrams", n = 2) %>%
separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
# remove stop words from tidytext package
anti_join(stop_words, by = c("first" = "word")) %>%
anti_join(stop_words, by = c("second" = "word")) %>%
filter(str_detect(first, "[a-z]"),
str_detect(second, "[a-z]")) %>%
group_by(headline,author,date) %>%
count(bigram) %>%
arrange(-n)
